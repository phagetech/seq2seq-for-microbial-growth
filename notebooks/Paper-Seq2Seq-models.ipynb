{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59f5f6e-9f4a-4cac-adb5-3c2f6ba6b5f8",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence with attention for microbial growth prediction\n",
    "\n",
    "This notebook contains all functions needed to implement Seq2Seq+Attn model using Tensorflow/Keras.\n",
    "\n",
    "**The data is property of PhageLab Chile SpA, and hence it is not publicly available.** \n",
    "\n",
    "The models can be trained on any growth curve.\n",
    "\n",
    "Correspondence: lleon@pht.cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a514753-e470-4fd8-adf9-55ed8baee70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sktime -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed4a0e-8d0d-422a-922f-5cb175180fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, regularizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects, pad_sequences\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sktime.performance_metrics.forecasting import mean_squared_scaled_error, median_squared_scaled_error, mean_absolute_scaled_error, median_absolute_scaled_error, mean_squared_error, mean_absolute_error, MeanAbsolutePercentageError\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59332f6-6f78-4267-a66c-4e7ebf0dee5f",
   "metadata": {},
   "source": [
    "# Model implementation and other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba7045-377a-40b1-ba00-7f744d8f2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tstudent_ci(samples, n=100, l=0.95):\n",
    "    sample_mean = np.mean(samples, axis=1)\n",
    "    sample_std = np.std(samples, axis=1)\n",
    "    alpha = 1-l\n",
    "    t_critical = stats.t.ppf(1-alpha/2, df=3)\n",
    "    sem = sample_std/np.sqrt(n)\n",
    "    margin_of_error = t_critical*sem\n",
    "    lower_bound = np.squeeze(sample_mean-margin_of_error).T\n",
    "    upper_bound = np.squeeze(sample_mean+margin_of_error).T\n",
    "    return (lower_bound, upper_bound)\n",
    "\n",
    "class StackedGRU(Model):\n",
    "    def __init__(self, latent_dim, num_rnn=1):\n",
    "        super(StackedGRU, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_rnn = num_rnn\n",
    "\n",
    "        self.init_rnn = GRU(latent_dim, return_sequences=True, name='init_rnn')\n",
    "        if self.num_rnn:\n",
    "            self.stacked_rnn = Sequential(name=\"stacked_rnn\")\n",
    "            for i in range(num_rnn):\n",
    "                self.stacked_rnn.add(GRU(latent_dim, return_sequences=True))\n",
    "        \n",
    "        self.out_rnn = GRU(latent_dim, return_sequences=True, return_state=True, name='out_rnn')\n",
    "\n",
    "    def call(self, inputs, initial_state=None):\n",
    "        x = self.init_rnn(inputs, initial_state=initial_state)\n",
    "        if self.num_rnn>0:\n",
    "            x = self.stacked_rnn(x)\n",
    "        x = self.out_rnn(x)\n",
    "        return x\n",
    "    \n",
    "class DenseD(Model):\n",
    "    def __init__(self, out_dim, name, num_dense=0, extra_dense_dim=16, activation='linear'):\n",
    "        super(DenseD, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_dense = num_dense\n",
    "        self.extra_dense_dim = extra_dense_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        if num_dense > 0:\n",
    "            self.extra_dense = Sequential(name=\"extra_dense\")\n",
    "            for i in range(num_dense):\n",
    "                self.extra_dense.add(TimeDistributed(Dense(extra_dense_dim)))\n",
    "                self.extra_dense.add(Dropout(0.2))\n",
    "                \n",
    "        self.dense_out = TimeDistributed(Dense(self.out_dim, activation=self.activation))\n",
    "        self.correction = TimeDistributed(Dense(self.out_dim))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        if self.num_dense > 0:\n",
    "            x = self.extra_dense(x)\n",
    "        x = self.dense_out(x)\n",
    "        corr = self.correction(inputs)\n",
    "        return x+corr\n",
    "\n",
    "def seq2seq_with_attention(\n",
    "    input_shape, \n",
    "    dec_input_shape, \n",
    "    output_sequence_length, \n",
    "    latent_dim, \n",
    "    dropout_rate=0.2,\n",
    "    mask_value=-10\n",
    "):\n",
    "    \"\"\"\n",
    "    This function implements the seq2seq model with attention to predict growth curves.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # GRU Encoder part\n",
    "    encoder_inputs = Input(shape=input_shape, name=\"encoder_inputs\")    \n",
    "    encoder_inputs_masked = Masking(mask_value=mask_value)(encoder_inputs)\n",
    "    \n",
    "    # GRU block\n",
    "    encoder_gru = StackedGRU(latent_dim, 3)\n",
    "    encoder_outputs, enc_gru_state = encoder_gru(encoder_inputs_masked)\n",
    "    encoder_states = [enc_gru_state]\n",
    "    \n",
    "    # GRU Decoder part\n",
    "    decoder_inputs = Input(shape=(None, dec_input_shape[1]), name=\"decoder_inputs\")\n",
    "    decoder_inputs_masked = Masking(mask_value=-10)(decoder_inputs)\n",
    "    decoder_state_input = Input(shape=(latent_dim,), name=\"decoder_state_input\")\n",
    "    decoder_states_inputs = [decoder_state_input]\n",
    "\n",
    "    decoder_gru = GRU(\n",
    "        latent_dim, \n",
    "        return_sequences=True, \n",
    "        return_state=True, \n",
    "        name=\"dec_gru\"\n",
    "    )\n",
    "    decoder_gru_output, _ = decoder_gru(decoder_inputs_masked, initial_state=encoder_states)\n",
    "    # Student's parameters mu, sigma\n",
    "    decoder_dense_mean = DenseD(1, num_dense=0, name=\"dec_mean\", activation='sigmoid')\n",
    "    decoder_dense_log_var = DenseD(1, num_dense=0, name=\"dec_var\", activation='linear')\n",
    "\n",
    "    # Attention block\n",
    "    attention = Attention(name=\"attention\", use_scale=True, dropout=0.2, score_mode=\"dot\")\n",
    "    # the context\n",
    "    context_vector = attention([decoder_gru_output, encoder_outputs], use_causal_mask=True)\n",
    "    decoder_combined_context = Concatenate(axis=-1, name=\"dec_comb_context\")([decoder_gru_output, context_vector])\n",
    "    dropout_layer = Dropout(0.2)\n",
    "    decoder_combined_context = dropout_layer(decoder_combined_context)\n",
    "    \n",
    "    # apply functions for mu and sigma\n",
    "    decoder_mean = decoder_dense_mean(decoder_combined_context)\n",
    "    decoder_log_var = decoder_dense_log_var(decoder_combined_context)\n",
    "    \n",
    "    # concatenate the results into a single output\n",
    "    decoder_outputs = Concatenate(axis=-1, name=\"dec_outputs\")([decoder_mean, decoder_log_var])\n",
    "    \n",
    "    # The full model\n",
    "    full_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    # Encoder model\n",
    "    encoder_model = Model(encoder_inputs, [encoder_outputs]+encoder_states)\n",
    "    \n",
    "    # Decoder model (single prediction)\n",
    "    # Here we reuse the decoder layer and the attention\n",
    "    decoder_inputs_single = Input(shape=(1, dec_input_shape[1]), name=\"dec_input_single\")\n",
    "    decoder_inputs_single_masked = Masking(mask_value=-10)(decoder_inputs_single)\n",
    "    encoder_outputs_input = Input(shape=(None, latent_dim), name=\"encoder_outputs_single\")\n",
    "\n",
    "    decoder_gru_output_single, dec_gru_state_single = decoder_gru(decoder_inputs_single_masked, initial_state=decoder_states_inputs)\n",
    "    context_vector_single = attention([decoder_gru_output_single, encoder_outputs_input], use_causal_mask=True)\n",
    "    decoder_combined_context_single = Concatenate(axis=-1)([decoder_gru_output_single, context_vector_single])\n",
    "    \n",
    "    decoder_mean_single = decoder_dense_mean(decoder_combined_context_single)\n",
    "    decoder_log_var_single = decoder_dense_log_var(decoder_combined_context_single)\n",
    "    decoder_outputs_single = Concatenate(axis=-1, name=\"decoder_outputs_single\")([decoder_mean_single, decoder_log_var_single])\n",
    "    \n",
    "    decoder_states = [dec_gru_state_single]\n",
    "    decoder_model = Model([decoder_inputs_single, encoder_outputs_input] + decoder_states_inputs, [decoder_outputs_single] + decoder_states)\n",
    "    \n",
    "    return full_model, encoder_model, decoder_model\n",
    "\n",
    "def simple_seq2seq(input_shape, dec_input_shape, output_sequence_length, latent_dim, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    This function implements the simple seq2seq model to predict growth curves.\n",
    "    \n",
    "    \"\"\"\n",
    "    # GRU Encoder part\n",
    "    encoder_inputs = Input(shape=input_shape, name=\"encoder_inputs\")\n",
    "    encoder_inputs_masked = Masking(mask_value=-10)(encoder_inputs)\n",
    "    \n",
    "    # GRU block\n",
    "    encoder_gru = StackedGRU(latent_dim, 3)\n",
    "    encoder_outputs, enc_gru_state = encoder_gru(encoder_inputs_masked)\n",
    "    encoder_states = [enc_gru_state]\n",
    "    \n",
    "    # GRU Decoder part\n",
    "    decoder_inputs = Input(shape=(None, dec_input_shape[1]), name=\"decoder_inputs\")\n",
    "    decoder_inputs_masked = Masking(mask_value=-10)(decoder_inputs)\n",
    "    decoder_state_input = Input(shape=(latent_dim,), name=\"decoder_state_input\")\n",
    "    decoder_states_inputs = [decoder_state_input]\n",
    "\n",
    "    decoder_gru = GRU(\n",
    "        latent_dim, \n",
    "        return_sequences=True, \n",
    "        return_state=True, \n",
    "        name=\"dec_gru\"\n",
    "    )\n",
    "    decoder_gru_output, _ = decoder_gru(decoder_inputs_masked, initial_state=encoder_states)\n",
    "\n",
    "    # Student's parameters mu, sigma\n",
    "    decoder_dense_mean = DenseD(1, num_dense=0, name=\"dec_mean\", activation='sigmoid')\n",
    "    decoder_dense_log_var = DenseD(1, num_dense=0, name=\"dec_var\", activation='linear')\n",
    "    \n",
    "    # apply functions for mu and sigma\n",
    "    decoder_mean = decoder_dense_mean(decoder_gru_output)\n",
    "    decoder_log_var = decoder_dense_log_var(decoder_gru_output)\n",
    "    \n",
    "    # concatenate the results into a single output\n",
    "    decoder_outputs = Concatenate(axis=-1, name=\"dec_outputs\")([decoder_mean, decoder_log_var])\n",
    "    \n",
    "    # The full model\n",
    "    full_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    # Encoder model\n",
    "    encoder_model = Model(encoder_inputs, [encoder_outputs]+encoder_states)\n",
    "    \n",
    "    # Decoder model (single prediction)\n",
    "    # Here we reuse the decoder layer\n",
    "    decoder_inputs_single = Input(shape=(1, dec_input_shape[1]), name=\"dec_input_single\")\n",
    "    decoder_inputs_single_masked = Masking(mask_value=-10)(decoder_inputs_single)\n",
    "    encoder_outputs_input = Input(shape=(None, latent_dim), name=\"encoder_outputs_single\")\n",
    "\n",
    "    decoder_gru_output_single, dec_gru_state_single = decoder_gru(decoder_inputs_single_masked, initial_state=decoder_states_inputs)\n",
    "    \n",
    "    decoder_mean_single = decoder_dense_mean(decoder_gru_output_single)\n",
    "    decoder_log_var_single = decoder_dense_log_var(decoder_gru_output_single)\n",
    "    decoder_outputs_single = Concatenate(axis=-1, name=\"decoder_outputs_single\")([decoder_mean_single, decoder_log_var_single])\n",
    "    \n",
    "    decoder_states = [dec_gru_state_single]\n",
    "    decoder_model = Model([decoder_inputs_single, encoder_outputs_input] + decoder_states_inputs, [decoder_outputs_single] + decoder_states)\n",
    "    \n",
    "    return full_model, encoder_model, decoder_model\n",
    "\n",
    "def forecast_probabilistic_gru(\n",
    "    encoder_model, \n",
    "    decoder_model, \n",
    "    input_sequence, \n",
    "    output_sequence_length, \n",
    "    n_samples,\n",
    "    n_features, \n",
    "    latent_dim, \n",
    "    n_realizations=100,\n",
    "    start_token=0.5,\n",
    "    t_student=False\n",
    "):\n",
    "    encoder_outputs, encoder_states = encoder_model.predict(input_sequence, verbose=0)\n",
    "    encoder_states = [encoder_states]\n",
    "\n",
    "    target_seq = start_token*np.ones((n_samples, 1, n_features))\n",
    "\n",
    "    decoded_sequence = []\n",
    "\n",
    "    for _ in range(output_sequence_length):\n",
    "        output_tokens, decoder_states = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + encoder_states,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        mean = output_tokens[..., 0]\n",
    "        log_var = output_tokens[..., 1]\n",
    "        var = np.exp(log_var)\n",
    "\n",
    "        if t_student:\n",
    "            samples = [stats.t.rvs(3, loc=mean, scale=np.sqrt(var)) for _ in range(n_realizations)]\n",
    "            samples = np.array(samples)\n",
    "        else:\n",
    "            samples = [np.random.normal(mean, np.sqrt(var)) for _ in range(n_realizations)]\n",
    "            samples = np.array(samples)\n",
    "\n",
    "        decoded_sequence.append(samples)\n",
    "\n",
    "        target_seq = np.zeros((n_samples, 1, n_features))\n",
    "        target_seq[:, 0, 0] = np.squeeze(mean)\n",
    "\n",
    "        encoder_states = [decoder_states]\n",
    "\n",
    "    return np.array(decoded_sequence)\n",
    "\n",
    "def student_t_nll(y_true, y_pred):\n",
    "    eps = 1e-8\n",
    "    mu = y_pred[..., 0]\n",
    "    log_sigma = y_pred[..., 1]\n",
    "    sigma = tf.exp(log_sigma)+eps  # Ensure sigma is positive    \n",
    "    nu = 3.0 # dof\n",
    "    term1 = y_pred[..., 1]  # Log of the scale\n",
    "    term2 = (nu+1.0)/2.0*K.log(1+(1/nu)*K.square((y_true[..., 0]-mu)/sigma))\n",
    "    nll = term1+term2\n",
    "    return K.mean(nll)\n",
    "\n",
    "def create_test_sequences(data, input_seq_len, output_seq_len, start_token=0.0):\n",
    "    n_samples, time, n_features = data.shape\n",
    "    X_encoder, X_decoder, y = [], [], []\n",
    "\n",
    "    for sample in range(n_samples):\n",
    "        for i in range(time - input_seq_len - output_seq_len + 1):\n",
    "            input_seq = data[sample, i:i+input_seq_len]\n",
    "            output_seq = data[sample, i+input_seq_len:i+input_seq_len+output_seq_len]\n",
    "            X_encoder.append(input_seq)\n",
    "            X_decoder.append(output_seq)\n",
    "            y.append(output_seq)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X_encoder = np.array(X_encoder)\n",
    "    X_decoder = np.array(X_decoder)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Shift X_decoder\n",
    "    X_decoder = np.concatenate([start_token*np.ones((X_decoder.shape[0], 1, n_features)), X_decoder[:, :-1, :]], axis=1)\n",
    "\n",
    "    return X_encoder, X_decoder, y\n",
    "\n",
    "def variable_length_per_sample(\n",
    "    time_series_data, \n",
    "    padding_value=-10,\n",
    "    start_token=-100,\n",
    "    lowl=36,\n",
    "    highl=48,\n",
    "    use_max_samps=False,\n",
    "    max_samps=5000,\n",
    "    sample_weight=None,\n",
    "    diff=False\n",
    "):\n",
    "\n",
    "    # Determine the number of samples and length of each series\n",
    "    num_samples, series_length, num_features = time_series_data.shape\n",
    "    data = np.squeeze(time_series_data)\n",
    "\n",
    "    # Lists to hold variable-length sequences\n",
    "    encoder_input_data_list = []\n",
    "    decoder_input_data_list = []\n",
    "    decoder_target_data_list = []\n",
    "\n",
    "    split_index = [12, 36, 42, 48, 96]\n",
    "    for series in data:\n",
    "        for si in split_index:\n",
    "            # Encoder input sequence\n",
    "            encoder_input = series[:si]\n",
    "            if diff:\n",
    "                encoder_diff = np.zeros_like(encoder_input)\n",
    "                encoder_diff[1:] = np.diff(encoder_input)\n",
    "                encoder_input = np.concatenate((encoder_input[:, None], encoder_diff[:, None]), axis=1)\n",
    "            encoder_input_data_list.append(encoder_input)\n",
    "\n",
    "            # Decoder input sequence\n",
    "            decoder_input = np.concatenate(([start_token], series[si:-1]))\n",
    "            decoder_input_data_list.append(decoder_input)\n",
    "\n",
    "            # Decoder target sequence is the decoder input shifted by one\n",
    "            decoder_target = series[si:]\n",
    "            decoder_target_data_list.append(decoder_target)\n",
    "\n",
    "    # Pad sequences to the maximum length in their respective lists\n",
    "    encoder_input_data = pad_sequences(\n",
    "        encoder_input_data_list, \n",
    "        padding='post', \n",
    "        value=padding_value, \n",
    "        dtype=data.dtype\n",
    "    )\n",
    "    decoder_input_data = pad_sequences(\n",
    "        decoder_input_data_list, \n",
    "        padding='post', \n",
    "        value=padding_value, \n",
    "        dtype=data.dtype\n",
    "    )\n",
    "    decoder_target_data = pad_sequences(\n",
    "        decoder_target_data_list, \n",
    "        padding='post', \n",
    "        value=padding_value, \n",
    "        dtype=data.dtype\n",
    "    )\n",
    "\n",
    "    # shuffle\n",
    "    idx = np.arange(len(encoder_input_data))\n",
    "    np.random.shuffle(idx)\n",
    "    encoder_input_data = encoder_input_data[idx]\n",
    "    decoder_input_data = decoder_input_data[idx]\n",
    "    decoder_target_data = decoder_target_data[idx]\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = np.repeat(sample_weight, len(split_index))\n",
    "        sample_weight = sample_weight[idx]\n",
    "    \n",
    "\n",
    "    # masks\n",
    "    encoder_mask = encoder_input_data!=padding_value\n",
    "    decoder_mask = decoder_input_data!=padding_value\n",
    "    target_mask = decoder_target_data!=padding_value\n",
    "    \n",
    "    if not diff:\n",
    "        encoder_input_data = np.expand_dims(encoder_input_data, -1)\n",
    "    decoder_input_data = np.expand_dims(decoder_input_data, -1)\n",
    "    decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
    "        \n",
    "\n",
    "    if use_max_samps:\n",
    "        return (\n",
    "            encoder_input_data[:max_samps], \n",
    "            decoder_input_data[:max_samps], \n",
    "            decoder_target_data[:max_samps],\n",
    "            encoder_mask[:max_samps],\n",
    "            decoder_mask[:max_samps],\n",
    "            target_mask[:max_samps]\n",
    "        )\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        return (\n",
    "        encoder_input_data, \n",
    "        decoder_input_data, \n",
    "        decoder_target_data,\n",
    "        encoder_mask,\n",
    "        decoder_mask,\n",
    "        target_mask,\n",
    "        sample_weight\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        encoder_input_data, \n",
    "        decoder_input_data, \n",
    "        decoder_target_data,\n",
    "        encoder_mask,\n",
    "        decoder_mask,\n",
    "        target_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c53982-b5cb-45bb-8620-64462bb1695e",
   "metadata": {},
   "source": [
    "# Random input data\n",
    "Due to the privacy of the data generated by PhageLab Chile SpA, we create random data for the purpose of running this script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24edfe0-45aa-4b79-870a-98649086509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "L = 109\n",
    "feats = 1\n",
    "shape = (N, L, feats)\n",
    "X = np.random.normal(size=shape)\n",
    "categories = np.random.randint(0, 4, size=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd975ee-7824-4f1e-960d-72f831274e65",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ed4d0-243e-49f0-8bd8-32206ef3510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "num_features = 1\n",
    "latent_dim = 128  \n",
    "lr = 1e-3\n",
    "epochs = 5\n",
    "t_student = True\n",
    "low_lim = 12\n",
    "up_lim = 96\n",
    "start_token = -100\n",
    "n_folds = 3\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2009023-b5ec-45d3-b0a2-8d4b9e9b8e5e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_per_fold = {}\n",
    "scalers = {}\n",
    "test_input_length = 36\n",
    "total_length = 109\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, categories)):\n",
    "    fold_name = f\"fold_{i}\"\n",
    "    \n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    \n",
    "    scaler = QuantileTransformer(output_distribution=\"normal\", n_quantiles=1000).fit(X_train.flatten()[:, None])\n",
    "    X_train = np.array(list(map(scaler.transform, X_train)))\n",
    "    X_test = np.array(list(map(scaler.transform, X_test)))\n",
    "    \n",
    "    # Create sequences    \n",
    "    X_encoder_train, X_decoder_train, y_train, encoder_mask_train, decoder_mask_train, y_mask_train = variable_length_per_sample(X_train, lowl=low_lim, highl=up_lim)\n",
    "    \n",
    "    X_encoder_test, X_decoder_test, y_test = create_test_sequences(\n",
    "        X_test, \n",
    "        test_input_length, \n",
    "        total_length-test_input_length,\n",
    "        start_token=start_token\n",
    "    )\n",
    "    # pad encoder_test to make it compatible with the model\n",
    "    X_encoder_test = np.pad(\n",
    "        X_encoder_test, \n",
    "        pad_width=((0,0),(0,up_lim-test_input_length),(0,0)), \n",
    "        mode='constant', \n",
    "        constant_values=-10\n",
    "    )\n",
    "    \n",
    "    input_shape_masked = (X_encoder_train.shape[1], num_features)\n",
    "    dec_input_shape = (X_encoder_train.shape[1], num_features)\n",
    "    output_shape_masked = X_decoder_train.shape[1]\n",
    "    \n",
    "    # load architecture\n",
    "    model, encoder_model, decoder_model = seq2seq_with_attention(\n",
    "        input_shape_masked, \n",
    "        dec_input_shape,\n",
    "        output_shape_masked, \n",
    "        latent_dim=latent_dim\n",
    "    )\n",
    "\n",
    "    if t_student:\n",
    "        loss = student_t_nll\n",
    "    else:\n",
    "        loss = gaussian_nll_loss\n",
    "    opt = Adamax(\n",
    "        learning_rate=lr,\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        [X_encoder_train, X_decoder_train], \n",
    "        y_train, \n",
    "        epochs=epochs, \n",
    "        batch_size=128,\n",
    "    )\n",
    "    \n",
    "    # Eval\n",
    "    # Perform the forecast with probabilistic output and attention   \n",
    "    samples = forecast_probabilistic_gru(\n",
    "        encoder_model, \n",
    "        decoder_model, \n",
    "        X_encoder_test, \n",
    "        X_decoder_test.shape[1],\n",
    "        X_encoder_test.shape[0],\n",
    "        num_features, \n",
    "        latent_dim,\n",
    "        start_token=start_token,\n",
    "        t_student=t_student\n",
    "    )\n",
    "\n",
    "    # For metric estimation, we need to undo the scaling\n",
    "    X_decoder_test = scaler.inverse_transform(X_decoder_test.flatten()[:, None]).reshape(X_decoder_test.shape)\n",
    "    X_encoder_test = scaler.inverse_transform(X_encoder_test.flatten()[:, None]).reshape(X_encoder_test.shape)\n",
    "    samples = scaler.inverse_transform(samples.flatten()[:, None]).reshape(samples.shape)\n",
    "    \n",
    "    mean_prediction = np.squeeze(np.mean(samples, axis=1)).T\n",
    "    y_true = scaler.inverse_transform(y_test.flatten()[:, None]).reshape(y_test.shape)\n",
    "    y_true = np.squeeze(y_true)\n",
    "    ci_level = get_tstudent_ci(samples)\n",
    "    lower_bound, upper_bound = ci_level\n",
    "    \n",
    "    smape_cls = MeanAbsolutePercentageError(symmetric=True)\n",
    "    metrics_per_fold[fold_name] = {\n",
    "        'MAE': mean_absolute_error(y_true, mean_prediction),\n",
    "        'MSE': mean_squared_error(y_true, mean_prediction),\n",
    "        'SMAPE': smape_cls(y_true, mean_prediction),\n",
    "        'MASE': mean_absolute_scaled_error(y_true, mean_prediction, y_train=np.squeeze(X_decoder_test)),\n",
    "        'RMSSE': mean_squared_scaled_error(y_true, mean_prediction, y_train=np.squeeze(X_decoder_test)),\n",
    "    }\n",
    "    \n",
    "    scalers[fold_name] = minmax_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538bf78c-3d79-4868-beee-2e6896d3fd69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.14.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.14.1-gpu-py310-cu118-ubuntu20.04-sagemaker-v1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
